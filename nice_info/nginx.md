Ниже — обзор **NGINX** как реверс-прокси, балансировщика и HTTP-сервера, а также объяснение его роли в деплое приложений.

---

## 1. Что такое NGINX?

**NGINX** (читается «Энжин-Икс») — высокопроизводительный веб-сервер с открытым исходным кодом, фокусирующийся на:
- **Обработке большого числа соединений** (высокая конкурентность).
- **Реверс-прокси** (прокси-сервер, принимающий HTTP/HTTPS-запросы снаружи и перенаправляющий их на один или несколько серверов приложений).
- **Балансировке нагрузки** (load balancing).
- **HTTP-кэшировании**, статических файлов, работе со сценариями FastCGI и др.

Изначально NGINX разрабатывался для решения проблемы **C10K** (обслуживание 10 000 одновременных соединений) и с тех пор стал одним из самых популярных веб-серверов и реверс-прокси.

---

## 2. Роль NGINX в деплое приложения

### 2.1 Реверс-прокси (Reverse Proxy)

- **Суть**: NGINX слушает на 80/443 (HTTP/HTTPS) порту «лицевой» трафик снаружи и «проксирует» запросы к вашему приложению (например, Flask, Django, Node.js), которое может работать на внутреннем порту (например, 8000).  
- **Зачем**:
  1. **Безопасность**: наружу не открываем прямой порт приложения, всё идёт через NGINX.
  2. **Удобство**: можно объединять несколько приложений под одним доменом, маршрутизируя запросы по «location» или поддоменам.
  3. **Terminating SSL/TLS**: NGINX может обрабатывать шифрование (HTTPS), а до приложения передавать незашифрованный трафик — упрощает логику в приложении.

### 2.2 Балансировка нагрузки (Load Balancing)

- **Суть**: Если у вас несколько бэкенд-серверов (приложений), NGINX может распределять входящие HTTP-запросы между ними.  
- **Типы балансировки**:
  1. **Round Robin**: по очереди на каждый сервер.
  2. **Least Connections**: отправлять запросы на сервер с наименьшим числом активных соединений.
  3. **IP Hash**: на основании IP-клиента запросы к одному и тому же серверу (полезно для session stickiness).

### 2.3 Кэширование (Caching)

- NGINX умеет кэшировать ответы от бэкенда, чтобы уменьшать нагрузку на приложение.
- Можно задать **proxy_cache**, время жизни кэша, условия сброса и т. д.

### 2.4 Обработка статического контента

- Один из плюсов NGINX — **очень быстрая** раздача статических файлов (CSS, JS, изображения).
- В продакшене обычно отделяют статику от динамики:
  - **NGINX** обслуживает статические файлы сам, без участия приложения.
  - Приложение обрабатывает только динамические запросы.

### 2.5 Завершение SSL (SSL termination)

- Часто NGINX выступает как **SSL-терминатор**:
  - Снаружи — HTTPS (SSL/TLS), сертификаты, ключи.
  - Внутри — HTTP или HTTPS к самому приложению.
- Это упрощает конфигурацию приложения, позволяя ему не заниматься шифрованием напрямую.

---

## 3. Почему именно NGINX?

1. **Высокая производительность**:
   - Использует асинхронную, событийную архитектуру (эполет, poll, kqueue), позволяющую обслуживать тысячи соединений в одном или нескольких рабочих процессах (worker processes).
2. **Низкое потребление ресурсов**:
   - В отличие от классических серверов (например, Apache в prefork-режиме), NGINX не порождает процесс/поток на каждое соединение.
3. **Широкие возможности настройки** (конфигурационные файлы `nginx.conf`, `sites-available`, `sites-enabled`).
4. **Широкая поддержка комьюнити** и множество готовых рецептов деплоя.

---

## 4. Пример конфигурации (Reverse Proxy + SSL)

Ниже — упрощённый пример, как настроить NGINX для приёма HTTPS-трафика и проксирования на локальное приложение (listening on 127.0.0.1:8000):

```nginx
# /etc/nginx/sites-available/myapp.conf

server {
    listen 80;
    server_name example.com;

    # Перенаправляем HTTP на HTTPS
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name example.com;

    # Путь к сертификату и ключу
    ssl_certificate     /etc/ssl/certs/example.crt;
    ssl_certificate_key /etc/ssl/private/example.key;

    # Раздача статических файлов
    location /static/ {
        alias /var/www/myapp/static/;
    }

    # Проксируем все остальные запросы к приложению
    location / {
        proxy_pass         http://127.0.0.1:8000;
        proxy_http_version 1.1;
        proxy_set_header   X-Real-IP $remote_addr;
        proxy_set_header   Host      $host;
        proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

- **Что происходит**:
  1. Входящий трафик на порт 80 перенаправляется на 443 (HTTPS).
  2. На 443 — NGINX принимает SSL и расшифровывает трафик.
  3. Если путь `/static/`, то NGINX отдает статические файлы напрямую.
  4. Все остальные запросы идут на `127.0.0.1:8000`, где работает бэкенд-приложение (например, Gunicorn + Flask).

---

## 5. Пример конфигурации (Load Balancing)

```nginx
# upstream блок объявляет группу серверов-приложений
upstream myapp_servers {
    server 10.0.0.2:8000;
    server 10.0.0.3:8000;
    # можно указать weight=2, max_fails и т.д.
}

server {
    listen 80;
    server_name example.com;

    location / {
        proxy_pass http://myapp_servers;
    }
}
```

- **Что происходит**:
  - NGINX равномерно распределяет запросы по двум серверам `10.0.0.2:8000` и `10.0.0.3:8000`.
  - При падении одного из серверов можно настроить health-check, retry и т. д.

---

## 6. NGINX в окружении Docker/Kubernetes

- **Docker**: 
  - Часто приложение и NGINX разворачиваются в **разных контейнерах**.  
  - NGINX-конфигурация указывает на Docker-сеть, где «живет» бэкенд.  
  - Пример docker-compose: 
    ```yaml
    version: '3'
    services:
      nginx:
        image: nginx:stable
        volumes:
          - ./nginx.conf:/etc/nginx/conf.d/default.conf
        ports:
          - "80:80"
          - "443:443"
        depends_on:
          - app

      app:
        build: .
        expose:
          - "8000"
        ...
    ```
- **Kubernetes**: 
  - Можно использовать **NGINX Ingress**-контроллер, который перенаправляет внешний трафик внутрь K8s к сервисам.
  - Это стандартный паттерн «Ingress Controller» в Kubernetes.

---

## 7. Вывод

1. **NGINX** играет ключевую роль в деплое:
   - Выступает **реверс-прокси** перед приложением, принимая и распределяя HTTP/HTTPS.
   - Может **балансировать нагрузку** между несколькими бэкендами.
   - Обеспечивает **SSL-терминацию**, **раздачу статических файлов**, **кэширование**.
2. **Широко используется** во множестве сценариев деплоя (Docker, Kubernetes, bare-metal).
3. Является **быстрым и надёжным** решением для высоконагруженных сервисов.

Таким образом, при деплое веб-приложения, NGINX часто является «первой точкой входа» для клиентов: он принимает трафик, выбирает бэкенд-сервер, обрабатывает статику и может шифровать/расшифровывать запросы (HTTPS).